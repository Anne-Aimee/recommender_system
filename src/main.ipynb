{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import scipy.io\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg import spsolve\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../utilities/')\n",
    "from helpers import *\n",
    "from plots import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load of training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ID indicates both the user and the item : user are identified as \"r\"+\"index\" from 1 to 10000, and items as \"c\"+ \"index\" from 1 to 1000. The format of the ID are consequently : r index(user) _ c index(item).\n",
    "The ratings are given as integers from 1 to 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute them into a matrix with items as row and users as columns. All of the missing ratings are set as 0 and should be predicted to have a valid rating format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = '../Datasets/data_train.csv'\n",
    "ratings = load_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the number of ratings per movie and user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_items_per_user, num_users_per_item = plot_raw_data(ratings)\n",
    "\n",
    "print(\"min # of items per user = {}, min # of users per item = {}.\".format(\n",
    "        min(num_items_per_user), min(num_users_per_item)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing - Split the data into a train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(ratings, num_items_per_user, num_users_per_item,\n",
    "               min_num_ratings, p_test=0.1):\n",
    "    \"\"\"split the ratings to training data and test data.\n",
    "    Args:\n",
    "        min_num_ratings: \n",
    "            all users and items we keep must have at least min_num_ratings per user and per item. \n",
    "    \"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(988)\n",
    "    \n",
    "    # select user and item based on the condition.\n",
    "    valid_users = np.where(num_items_per_user >= min_num_ratings)[0]\n",
    "    valid_items = np.where(num_users_per_item >= min_num_ratings)[0]\n",
    "    valid_ratings = ratings[valid_items, :][: , valid_users]  \n",
    "    \n",
    "    # split the data and return train and test data.\n",
    "    # we only consider users and movies that have more than 10 ratings\n",
    "\n",
    "    ind_test = np.random.choice(valid_ratings.nnz, int(valid_ratings.nnz*p_test), replace=False)\n",
    "    ind_train = np.delete(np.arange(valid_ratings.nnz),ind_test)\n",
    "    \n",
    "    valid_ratings_coo = valid_ratings.tocoo()\n",
    "    data = valid_ratings_coo.data\n",
    "    row = valid_ratings_coo.row\n",
    "    col = valid_ratings_coo.col\n",
    "    \n",
    "    test = sp.coo_matrix((data[ind_test], (row[ind_test], col[ind_test])), shape=valid_ratings.get_shape())\n",
    "    train = sp.coo_matrix((data[ind_train], (row[ind_train], col[ind_train])), shape=valid_ratings.get_shape()) \n",
    "    \n",
    "    print(\"Total number of nonzero elements in origial data:{v}\".format(v=ratings.nnz))\n",
    "    print(\"Total number of nonzero elements in train data:{v}\".format(v=train.nnz))\n",
    "    print(\"Total number of nonzero elements in test data:{v}\".format(v=test.nnz))\n",
    "    return valid_ratings, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ratings, train, test = split_data(\n",
    "    ratings, num_items_per_user, num_users_per_item, min_num_ratings=10, p_test=0.1)\n",
    "plot_train_test_data(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Baselines "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the global mean to do the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_global_mean(train, test):\n",
    "    \"\"\"baseline method: use the global mean.\"\"\"  \n",
    "    \n",
    "    return sqrt(calculate_mse(test.data,np.mean(train.data))/(test.nnz))\n",
    "\n",
    "baseline_global_mean(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the user means as the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_user_mean(train, test):\n",
    "    \"\"\"baseline method: use the user means as the prediction.\"\"\"\n",
    "    mse = 0\n",
    "    num_items, num_users = train.shape\n",
    "    \n",
    "    #Sum over nth user\n",
    "    sum_ratings_movie = np.squeeze(np.asarray(train.sum(0)))    # sum of the nonzero elements, for each row\n",
    "    count_ratings_movie = np.diff(train.tocsc().indptr)         # count of the nonzero elements, for each row\n",
    "    mean_rating_movie = sum_ratings_movie/count_ratings_movie\n",
    "    return sqrt(calculate_mse(test.data,mean_rating_movie[test.col])/(test.nnz))\n",
    "\n",
    "baseline_user_mean(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the item means as prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_item_mean(train, test):\n",
    "    \"\"\"baseline method: use item means as the prediction.\"\"\"\n",
    "    mse = 0\n",
    "    num_items, num_users = train.shape\n",
    "    \n",
    "    #Sum over dth movie\n",
    "    sum_ratings_user = np.squeeze(np.asarray(train.sum(1)))    # sum of the nonzero elements, for each row\n",
    "    count_ratings_user = np.diff(train.tocsr().indptr)         # count of the nonzero elements, for each row\n",
    "    mean_rating_user = sum_ratings_user/count_ratings_user\n",
    "\n",
    "    return sqrt(calculate_mse(test.data,mean_rating_user[test.row])/(test.nnz))\n",
    "    \n",
    "baseline_item_mean(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the item/user means as prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import spsolve\n",
    "\n",
    "def baseline_item_user(train, test):\n",
    "    \"\"\"baseline method: find best parameters for the model y_dn = w_0 + w_item[d] + w_user[n] (D+N+1) parameters\n",
    "       and make a prediction.\"\"\"\n",
    "    \n",
    "    global_mean = np.mean(train.data)\n",
    "    \n",
    "    #Sum over nth user\n",
    "    sum_ratings_movie = np.squeeze(np.asarray(train.sum(0)))    # sum of the nonzero elements, for each row\n",
    "    count_ratings_movie = np.diff(train.tocsc().indptr)         # count of the nonzero elements, for each row\n",
    "    \n",
    "    #Sum over dth movie\n",
    "    sum_ratings_user = np.squeeze(np.asarray(train.sum(1)))    # sum of the nonzero elements, for each col\n",
    "    count_ratings_user = np.diff(train.tocsr().indptr)         # count of the nonzero elements, for each col\n",
    "    \n",
    "    num_items, num_users = train.shape\n",
    "    \n",
    "    # Constructing linear system defining the model's optimal parameters in form of a matrix\n",
    "    \n",
    "    # Matrix of the same shape as ratings, 1 if rating present, 0 otherwise\n",
    "    mask_train = sp.coo_matrix((np.ones(train.nnz), (train.row, train.col)), shape=train.shape) \n",
    "    \n",
    "    A = sp.hstack((sp.diags(count_ratings_user), mask_train))\n",
    "    A = sp.vstack((A, sp.hstack((mask_train.T, sp.diags(count_ratings_movie)))))\n",
    "    A = sp.hstack((A, sp.coo_matrix(np.concatenate((count_ratings_movie,count_ratings_user))).T))\n",
    "    A = sp.vstack((A, sp.coo_matrix(np.ones(num_items+num_users+1))))\n",
    "    \n",
    "    b = np.append(np.concatenate((sum_ratings_user, sum_ratings_movie)),global_mean)\n",
    "    \n",
    "    # Solving the system\n",
    "    x = spsolve(A.tocsc(),b)\n",
    "    \n",
    "    # Extracting the parameters w_0, w_item[d] and w_user[n] \n",
    "    w_item, w_user, w_0 = np.split(x,np.array([num_items,num_items+num_users]))\n",
    "    \n",
    "    # Plot prediction versus test values\n",
    "    plt.title(\"predicted vs actual ratings\")\n",
    "    plt.xlabel(\"predicted ratings\")\n",
    "    plt.ylabel(\"actual ratings\")\n",
    "    plt.scatter(w_item[test.row] + w_user[test.col] + w_0, test.data,s=0.1)\n",
    "    \n",
    "    return sqrt(calculate_mse(test.data, w_item[test.row] + w_user[test.col] + w_0)/(test.nnz))\n",
    "    \n",
    "baseline_item_user(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn the Matrix Factorization using SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize matrix factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_MF(train, num_features):\n",
    "    \"\"\"init the parameter for matrix factorization.\"\"\"\n",
    "    \n",
    "    # returns initialized with random values :\n",
    "    #     user_features: shape = num_features, num_user\n",
    "    #     item_features: shape = num_features, num_item\n",
    "\n",
    "    \n",
    "    max_initial_value = 2*sqrt(np.mean(train.data)/num_features)\n",
    "    \n",
    "    user_features = max_initial_value*np.random.rand(num_features, train.shape[1])\n",
    "    item_features = max_initial_value*np.random.rand(num_features, train.shape[0])\n",
    "\n",
    "    \n",
    "    return user_features,item_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error(data, user_features, item_features, nz):\n",
    "    \"\"\"compute the loss (MSE) of the prediction of nonzero elements.\"\"\"\n",
    "\n",
    "    # calculate rmse (we only consider nonzero entries.)\n",
    "    approx_data_matrix = np.dot(item_features.T,user_features)\n",
    "    return sqrt(calculate_mse(data,approx_data_matrix[nz])/(len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error(data, user_features, item_features, nz):\n",
    "    \"\"\"compute the loss (MSE) of the prediction of nonzero elements.\"\"\"\n",
    "\n",
    "    # calculate rmse (we only consider nonzero entries.)\n",
    "    approx_data_matrix = np.dot(item_features.T,user_features)\n",
    "    return sqrt(calculate_mse(data,approx_data_matrix[nz])/(len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_pred(user_features,item_features):\n",
    "    \"\"\"clips the prediction of ratings below 1 to 1 or above 5 to 5 and inbetween clips \n",
    "     prediction near an integer to that integer.\"\"\"\n",
    "    approx_data_matrix = np.dot(item_features.T,user_features)\n",
    "    approx_data_matrix[approx_data_matrix>4.9]=5;\n",
    "    approx_data_matrix[approx_data_matrix<1.1]=1;\n",
    "    approx_data_matrix[np.all([approx_data_matrix>1.9, approx_data_matrix<2.1],axis=0)]=2;\n",
    "    approx_data_matrix[np.all([approx_data_matrix>2.9, approx_data_matrix<3.1],axis=0)]=3;\n",
    "    approx_data_matrix[np.all([approx_data_matrix>3.9, approx_data_matrix<4.1],axis=0)]=4;\n",
    "\n",
    "    return approx_data_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_clipped_error(data, user_features, item_features, nz):\n",
    "    \"\"\"compute the loss (MSE) of the clipped prediction of nonzero elements.\"\"\"\n",
    "    approx_data_matrix = clip_pred(user_features, item_features)\n",
    "    return sqrt(calculate_mse(data,approx_data_matrix[nz])/(len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix factorization SGD basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_factorization_SGD(train, test):\n",
    "    \"\"\"matrix factorization by SGD.\"\"\"\n",
    "    # define parameters\n",
    "    gamma = 0.05\n",
    "    num_features = 25   # K in the lecture notes\n",
    "    num_epochs = 30     # number of full iterations through the train set\n",
    "    errors = [0]\n",
    "    \n",
    "    # set seed\n",
    "    np.random.seed(988)\n",
    "\n",
    "    # init matrix\n",
    "    user_features, item_features = init_MF(train, num_features)\n",
    "    \n",
    "    # find the non-zero ratings indices \n",
    "    nz_row, nz_col = train.nonzero()\n",
    "    nz_train = list(zip(nz_row, nz_col, train.data))\n",
    "\n",
    "    print(\"learn the matrix factorization using SGD...\")\n",
    "    rmse_tr = compute_error(train.data, user_features, item_features, train.nonzero())\n",
    "    rmse_te = compute_clipped_error(test.data, user_features, item_features, test.nonzero())\n",
    "    print(\"initial RMSE on training set: {}, RMSE on testing set: {}.\".format(rmse_tr,rmse_te))\n",
    "    \n",
    "    for it in range(num_epochs):        \n",
    "        # shuffle the training rating indices\n",
    "        np.random.shuffle(nz_train)\n",
    "        \n",
    "        # decrease step size\n",
    "        gamma /= 1.2\n",
    "        \n",
    "        for d, n, x_dn in nz_train:\n",
    "        # update matrix factorization.     \n",
    "\n",
    "            item_features[:,d] += gamma*(x_dn - np.inner(item_features[:,d],user_features[:,n]))*user_features[:,n]\n",
    "            user_features[:,n] += gamma*(x_dn - np.inner(item_features[:,d],user_features[:,n]))*item_features[:,d]\n",
    "        \n",
    "        rmse_tr = compute_error(train.data, user_features, item_features, train.nonzero())\n",
    "        rmse_te = compute_clipped_error(test.data, user_features, item_features, test.nonzero())\n",
    "        print(\"iter: {}, RMSE on training set: {}, RMSE on testing set: {}.\".format(it, rmse_tr,rmse_te))\n",
    "        \n",
    "        errors.append(rmse_te)\n",
    "\n",
    "    # evaluate the test error.\n",
    "    rmse = compute_clipped_error(test.data, user_features, item_features, test.nonzero())\n",
    "    print(\"RMSE on test data: {}.\".format(rmse))\n",
    "\n",
    "matrix_factorization_SGD(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix factorization SGD regularized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_factorization_SGD_regularized(train, test, num_features, lambda_user, lambda_item, gamma, gamma_dec_step_size, num_epochs, seed, stop_criterion):\n",
    "    \"\"\"matrix factorization by SGD.\"\"\"\n",
    "    \n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # init matrix\n",
    "    user_features, item_features = init_MF(train, num_features)\n",
    "    \n",
    "    # find the non-zero ratings indices  \n",
    "    nz_train = list(zip(train.row, train.col, train.data))\n",
    "    \n",
    "    print(\"learn the matrix factorization using SGD...\")\n",
    "    rmse_tr = [compute_error(train.data, user_features, item_features, train.nonzero())]\n",
    "    rmse_te = [compute_clipped_error(test.data, user_features, item_features, test.nonzero())]\n",
    "    print(\"initial RMSE on training set: {}, RMSE on testing set: {}.\".format(rmse_tr[0],rmse_te[0]))\n",
    "    \n",
    "    for it in range(num_epochs):        \n",
    "        # shuffle the training rating indices\n",
    "        np.random.shuffle(nz_train)\n",
    "        \n",
    "        # decrease step size\n",
    "        gamma /= gamma_dec_step_size\n",
    "        \n",
    "        for d, n, x_dn in nz_train:\n",
    "        # update matrix factorization.\n",
    "\n",
    "            item_features[:,d] += gamma*((x_dn - np.inner(item_features[:,d],user_features[:,n]))*user_features[:,n]-lambda_item*item_features[:,d])\n",
    "            user_features[:,n] += gamma*((x_dn - np.inner(item_features[:,d],user_features[:,n]))*item_features[:,d]-lambda_user*user_features[:,n])\n",
    "        \n",
    "        rmse_tr.append(compute_error(train.data, user_features, item_features, train.nonzero()))\n",
    "        rmse_te.append(compute_clipped_error(test.data, user_features, item_features, test.nonzero()))\n",
    "        print(\"iter: {}, RMSE on training set: {}, RMSE on testing set: {}.\".format(it, rmse_tr[-1],rmse_te[-1]))\n",
    "        \n",
    "        if np.isclose(rmse_tr[-1],rmse_tr[-2],stop_criterion) or rmse_tr[-1] > rmse_tr[0]:\n",
    "            break\n",
    "            \n",
    "    # evaluate the test error.\n",
    "    min_rmse_te = min(rmse_te)\n",
    "    print(\"RMSE on test data: {}.\".format(min_rmse_te))\n",
    "    \n",
    "    return min_rmse_te\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters \n",
    "num_features = 40   # K in the lecture notes\n",
    "\n",
    "lambda_user = 0.08\n",
    "lambda_item = 0.08\n",
    "    \n",
    "gamma = 0.05\n",
    "gamma_dec_step_size = 1.2\n",
    "num_epochs = 30     # number of full passes through the train set\n",
    "stop_criterion = 1e-4\n",
    "    \n",
    "seed = 988\n",
    "\n",
    "matrix_factorization_SGD_regularized(train, test, num_features, lambda_user, lambda_item, gamma, gamma_dec_step_size, num_epochs, seed, stop_criterion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_simple_heatmap(data, title, xlabel, xticklabels, ylabel, yticklabels):\n",
    "\n",
    "    f,a = plt.subplots()\n",
    "    a.set_xlabel(xlabel)\n",
    "    a.set_xticks(range(len(xticklabels)))\n",
    "    a.set_xticklabels(xticklabels)\n",
    "    a.set_ylabel(ylabel)\n",
    "    a.set_yticks(range(len(yticklabels)))\n",
    "    a.set_yticklabels(yticklabels)\n",
    "    a.set_title(title)\n",
    "    heatmap_corr = a.imshow(data)\n",
    "    f.colorbar(heatmap_corr, ax=a)\n",
    "    \n",
    "data = np.random.rand(5,7)\n",
    "plot_simple_heatmap(data, \"title\", \"xlabel\",np.arange(7), \"ylabel\",np.arange(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_simple_heatmaps(data_1, data_2, fig_title, subtitle_1, subtitle_2, xlabel_shared, ylabel_shared):\n",
    "\n",
    "    f,a = plt.subplots(2,1)\n",
    "    \n",
    "    a[0].set_xlabel(xlabel_shared)\n",
    "    a[0].set_ylabel(ylabel_shared)\n",
    "    a[0].set_title(subtitle_1)\n",
    "    heatmap_0 = a[0].imshow(data_1)\n",
    "\n",
    "    a[1].set_xlabel(xlabel_shared)\n",
    "    a[1].set_ylabel(ylabel_shared)\n",
    "    a[1].set_title(subtitle_2)\n",
    "    heatmap_1 = a[1].imshow(data_2)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    f.colorbar(heatmap_0,ax=a[0])\n",
    "    f.colorbar(heatmap_1,ax=a[1])\n",
    "    \n",
    "    f.suptitle(fig_title)\n",
    "    \n",
    "data_1 = np.random.rand(200,300)\n",
    "data_2 = np.random.rand(200,300)\n",
    "plot_simple_heatmaps(data_1, data_2, 'fig_title', 'subtitle_1', 'subtitle_2', 'xlabel_shared', 'ylabel_shared')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the best initial gamma and the best decrement step size to compare SGD with ALS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# define parameters \n",
    "num_features = 25   # K in the lecture notes\n",
    "\n",
    "lambda_user = 0.08\n",
    "lambda_item = 0.08\n",
    "    \n",
    "gammas = np.logspace(-2,-1,num = 5)\n",
    "gamma_dec_step_sizes = np.linspace(1.1,1.3,5)\n",
    "\n",
    "\n",
    "num_epochs = 30     # number of full passes through the train set\n",
    "stop_criterion = 1e-4\n",
    "    \n",
    "seeds = np.array([988,1000])\n",
    "#seeds = np.array([988])\n",
    "\n",
    "rmse_te = np.zeros((len(seeds),len(gammas),len(gamma_dec_step_sizes)))\n",
    "\n",
    "for ind_seed, seed in enumerate(seeds):\n",
    "    for ind_gamma, gamma in enumerate(gammas):\n",
    "        for ind_gamma_dec_step_size, gamma_dec_step_size in enumerate(gamma_dec_step_sizes):\n",
    "            print(\"seed ({}/{}) = {}\".format(ind_seed+1, len(seeds), seed))\n",
    "            print(\"gamma ({}/{}) = {}\".format(ind_gamma+1, len(gammas), gamma))\n",
    "            print(\"stepsize decrement ({}/{}) = {}\".format(ind_gamma_dec_step_size+1, len(gamma_dec_step_sizes), gamma_dec_step_size))\n",
    "            \n",
    "            rmse_te[ind_seed,ind_gamma,ind_gamma_dec_step_size] = matrix_factorization_SGD_regularized(train, test, num_features, lambda_user, lambda_item, gamma, gamma_dec_step_size, num_epochs, seed, stop_criterion)\n",
    "\n",
    "np.save('../results_of_lengthy_computations/RMSE_test_tuning_gammas',rmse_te)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE_test_tuning_lambdas = np.load('../results_of_lengthy_computations/RMSE_test_tuning_gammas.npy')\n",
    "gammas = np.logspace(-2,-1,num = 5)\n",
    "gamma_dec_step_sizes = np.linspace(1.1,1.3,5)\n",
    "plot_simple_heatmap(np.min(RMSE_test_tuning_lambdas, axis = 0), \"RMSE\", \"decrement\",np.around(gamma_dec_step_sizes,2), \"gamma\",np.around(gammas,2))\n",
    "plt.savefig('../plots/heatmap_tuning_gammas.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn the Matrix Factorization using Alternating Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_user_feature(\n",
    "        train, item_features, lambda_user,\n",
    "        nnz_items_per_user, nz_user_itemindices):\n",
    "    \"\"\"update user feature matrix.\"\"\"\n",
    "    \"\"\"the best lambda is assumed to be nnz_items_per_user[user] * lambda_user\"\"\"\n",
    "\n",
    "    # update and return user feature.\n",
    "    user_features = np.zeros((item_features.shape[0],train.shape[1]))\n",
    "    \n",
    "    for n in range(train.shape[1]):\n",
    "        \n",
    "        item_features_n = np.zeros(item_features.shape)\n",
    "        item_features_n[:,nz_user_itemindices[n]] = item_features[:,nz_user_itemindices[n]]\n",
    "        user_features[:,n] = np.linalg.solve(np.dot(item_features_n,item_features.T)+lambda_user*nnz_items_per_user[n]*np.identity(user_features.shape[0]),np.dot(item_features,np.squeeze(np.asarray(train.getcol(n).todense()))))\n",
    "    \n",
    "    return user_features\n",
    "\n",
    "def update_item_feature(\n",
    "        train, user_features, lambda_item,\n",
    "        nnz_users_per_item, nz_item_userindices):\n",
    "    \"\"\"update item feature matrix.\"\"\"\n",
    "    \"\"\"the best lambda is assumed to be nnz_items_per_item[item] * lambda_item\"\"\"\n",
    "\n",
    "    # update and return item feature.\n",
    "    item_features = np.zeros((user_features.shape[0],train.shape[0]))\n",
    "    \n",
    "    for d in range(train.shape[0]):\n",
    "        \n",
    "        user_features_d = np.zeros(user_features.shape)\n",
    "        user_features_d[:,nz_item_userindices[d]] = user_features[:,nz_item_userindices[d]]\n",
    "        item_features[:,d] = np.linalg.solve(np.dot(user_features_d,user_features.T)+lambda_item*nnz_users_per_item[d]*np.identity(user_features.shape[0]),np.dot(user_features,np.squeeze(np.asarray(train.getrow(d).todense()))))\n",
    "    \n",
    "    return item_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ALS(train, test):\n",
    "    \"\"\"Alternating Least Squares (ALS) algorithm.\"\"\"\n",
    "    # define parameters\n",
    "    num_features = 25   # K in the lecture notes\n",
    "    lambda_user = 0.08\n",
    "    lambda_item = 0.08\n",
    "    stop_criterion = 1e-4\n",
    "    change = 1\n",
    "    error_list = [[0, 0]]\n",
    "    max_iter = 30\n",
    "    \n",
    "    # set seed\n",
    "    np.random.seed(988)\n",
    "\n",
    "    # init ALS\n",
    "    user_features, item_features = init_MF(train, num_features)\n",
    "    \n",
    "    # start you ALS-WR algorithm. \n",
    "    \n",
    "    nz_row, nz_col = train.nonzero()\n",
    "    \n",
    "    nz_user_itemindices = [nz_row[nz_col==n] for n in range(train.shape[1])]\n",
    "    nnz_items_per_user = np.array([len(nz_user_itemindice) for nz_user_itemindice in nz_user_itemindices])\n",
    "    nz_item_userindices = [nz_col[nz_row==d] for d in range(train.shape[0])]\n",
    "    nnz_users_per_item = np.array([len(nz_item_userindice) for nz_item_userindice in nz_item_userindices])\n",
    "    \n",
    "    rmse_tr = compute_error(train.data, user_features, item_features, train.nonzero())\n",
    "    rmse_te = compute_error(test.data, user_features, item_features, test.nonzero())\n",
    "    print(\"initial: RMSE on training set: {}, RMSE on testing set: {}.\".format(rmse_tr,rmse_te))\n",
    "    error_list.append([rmse_tr,rmse_te])\n",
    "    \n",
    "    it = 0\n",
    "    while (it < max_iter and not np.isclose(error_list[it][0],error_list[it+1][0],stop_criterion)):\n",
    "        it += 1\n",
    "        \n",
    "        user_features = update_user_feature(train, item_features, lambda_user, nnz_items_per_user, nz_user_itemindices)\n",
    "        item_features = update_item_feature(train, user_features, lambda_item, nnz_users_per_item, nz_item_userindices)\n",
    "        \n",
    "        rmse_tr = compute_error(train.data, user_features, item_features, train.nonzero())\n",
    "        rmse_te = compute_error(test.data, user_features, item_features, test.nonzero())\n",
    "        print(\"iter: {}, RMSE on training set: {}, RMSE on testing set: {}.\".format(it, rmse_tr,rmse_te))\n",
    "        \n",
    "        error_list.append([rmse_tr,rmse_te])\n",
    "        \n",
    "    rmse = compute_error(test.data, user_features, item_features, test.nonzero())\n",
    "    print(\"RMSE on test data: {}.\".format(rmse))\n",
    "       \n",
    "\n",
    "ALS(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the best ridge parameters using ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ALS(train, test, num_features, lambda_user, lambda_item, max_iter, seed):\n",
    "    \"\"\"Alternating Least Squares (ALS) algorithm.\"\"\"\n",
    "    # define parameters\n",
    "    \n",
    "    # set seed\n",
    "    np.random.seed(988)\n",
    "\n",
    "    # init ALS\n",
    "    user_features, item_features = init_MF(train, num_features)\n",
    "    \n",
    "    # start you ALS-WR algorithm. \n",
    "    \n",
    "    nz_row, nz_col = train.nonzero()\n",
    "    \n",
    "    nz_user_itemindices = [nz_row[nz_col==n] for n in range(train.shape[1])]\n",
    "    nnz_items_per_user = np.array([len(nz_user_itemindice) for nz_user_itemindice in nz_user_itemindices])\n",
    "    nz_item_userindices = [nz_col[nz_row==d] for d in range(train.shape[0])]\n",
    "    nnz_users_per_item = np.array([len(nz_item_userindice) for nz_item_userindice in nz_item_userindices])\n",
    "    \n",
    "    rmse_tr = [compute_error(train.data, user_features, item_features, train.nonzero())]\n",
    "    rmse_te = [compute_clipped_error(test.data, user_features, item_features, test.nonzero())]\n",
    "    print(\"initial: RMSE on training set: {}, RMSE on testing set: {}.\".format(rmse_tr[0],rmse_te[0]))\n",
    "    \n",
    "\n",
    "    for it in range(max_iter):\n",
    "        \n",
    "        user_features = update_user_feature(train, item_features, lambda_user, nnz_items_per_user, nz_user_itemindices)\n",
    "        item_features = update_item_feature(train, user_features, lambda_item, nnz_users_per_item, nz_item_userindices)\n",
    "        \n",
    "        \n",
    "        \n",
    "        rmse_tr.append(compute_error(train.data, user_features, item_features, train.nonzero()))\n",
    "        rmse_te.append(compute_clipped_error(test.data, user_features, item_features, test.nonzero()))\n",
    "        print(\"iter: {}, RMSE on training set: {}, RMSE on testing set: {}.\".format(it, rmse_tr[-1],rmse_te[-1]))\n",
    "        \n",
    "        if np.isclose(rmse_tr[-1],rmse_tr[-2],stop_criterion) or rmse_tr[-1] > rmse_tr[0]:\n",
    "            break\n",
    "        \n",
    "    min_rmse_te = min(rmse_te)\n",
    "    print(\"RMSE on test data: {}.\".format(min_rmse_te))\n",
    "    \n",
    "    return min_rmse_te\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# define parameters \n",
    "num_features = 25   # K in the lecture notes\n",
    "lambda_users = np.logspace(-2,0,num = 5)\n",
    "lambda_items = np.logspace(-2,0,num = 5)\n",
    "stop_criterion = 1e-4\n",
    "max_iter = 15\n",
    "    \n",
    "seeds = np.array([988])\n",
    "\n",
    "rmse_te = np.zeros((len(seeds),len(lambda_users),len(lambda_items)))\n",
    "\n",
    "for ind_seed, seed in enumerate(seeds):\n",
    "    for ind_lambda_user, lambda_user in enumerate(lambda_users):\n",
    "        for ind_lambda_item, lambda_item in enumerate(lambda_items):\n",
    "            print(\"seed ({}/{}) = {}\".format(ind_seed+1, len(seeds), seed))\n",
    "            print(\"lambda_user ({}/{}) = {}\".format(ind_lambda_user+1, len(lambda_users), lambda_user))\n",
    "            print(\"lambda_item ({}/{}) = {}\".format(ind_lambda_item+1, len(lambda_items), lambda_item))\n",
    "            \n",
    "            rmse_te[ind_seed,ind_lambda_user,ind_lambda_item] = ALS(train, test, num_features, lambda_user, lambda_item, max_iter, seed)\n",
    "\n",
    "np.save('../results_of_lengthy_computations/RMSE_test_tuning_lambdas',rmse_te)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE_test_tuning_lambdas = np.load('../results_of_lengthy_computations/RMSE_test_tuning_lambdas.npy')\n",
    "lambda_users = np.logspace(-2,0,num = 5)\n",
    "lambda_items = np.logspace(-2,0,num = 5)\n",
    "plot_simple_heatmap(RMSE_test_tuning_lambdas, \"RMSE\", \"lambda item\",np.around(lambda_items,2), \"lambda user\",np.around(lambda_users,2))\n",
    "plt.savefig('../plots/heatmap_tuning_lambdas.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- test and train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- kmean. surely not the most efficient method\n",
    "- matrix factorizations\n",
    "- neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../Datasets/sample_submission.csv'\n",
    "pred_submission =\n",
    "create_csv_submission(ids_test, pred_submission, OUTPUT_PATH)\n",
    "raise NotImplementedError"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
